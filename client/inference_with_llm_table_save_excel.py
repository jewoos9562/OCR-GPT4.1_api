import tritonclient.http as httpclient
import numpy as np
import os
import base64
from PIL import Image
from openai import OpenAI
from dotenv import load_dotenv
import pandas as pd
import re




# === ì„¤ì • ===
TRITON_URL = "localhost:8000"
MODEL_NAME = "ensemble"
input_image_path = "/workspace/sample_image2/insure/images/insure_00004.jpeg"
save_folder = "/workspace/result"
os.makedirs(save_folder, exist_ok=True)

# === Triton í´ë¼ì´ì–¸íŠ¸ ìƒì„± ===
client = httpclient.InferenceServerClient(url=TRITON_URL)

def encode_image_base64(image_path):
    with open(image_path, "rb") as f:
        encoded = base64.b64encode(f.read()).decode("utf-8")
        if len(encoded) % 4:
            encoded += "=" * (4 - len(encoded) % 4)
        return encoded

def decode_float(x):
    try:
        return float(x.decode("utf-8")) if isinstance(x, bytes) else float(x)
    except:
        return 0.0

# === Triton ì…ë ¥ ì„¤ì • ===
encoded_image = encode_image_base64(input_image_path)
inputs = httpclient.InferInput("input_image", [1], "BYTES")
inputs.set_data_from_numpy(np.array([encoded_image], dtype=object))
outputs = [
    httpclient.InferRequestedOutput("final_texts_with_line"),
    httpclient.InferRequestedOutput("html_tags")
]

# === Triton ì¶”ë¡  ===
response = client.infer(model_name=MODEL_NAME, inputs=[inputs], outputs=outputs)
texts = response.as_numpy("final_texts_with_line")
html = response.as_numpy("html_tags")

# === OCR í…ìŠ¤íŠ¸ ì •ë¦¬ ===
ocr_lines = []
for t in texts:
    x1, y1, x2, y2, text, line_num = t
    x1, y1, x2, y2 = map(decode_float, [x1, y1, x2, y2])
    text = text.decode("utf-8") if isinstance(text, bytes) else str(text)
    ocr_lines.append(f"[{x1:.1f}, {y1:.1f}, {x2:.1f}, {y2:.1f}] \"{text}\"")
ocr_text_block = "\n".join(ocr_lines)
html_text = "\n".join(line.decode("utf-8") for line in html)

# === OpenAI GPT ìš”ì²­ ===
load_dotenv()
openai_client = OpenAI()

prompt = f"""
ë‹¤ìŒì€ í•œ ì‹ ì²­ì„œ ë¬¸ì„œì—ì„œ ì¶”ì¶œëœ HTML í…Œì´ë¸”ê³¼ OCR í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤.

ì´ ë¬¸ì„œì—ì„œ ì¤‘ìš”í•œ ì •ë³´ë“¤ì„ key-value í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•´ì¤˜.
í•­ëª© ê°œìˆ˜ëŠ” ì •í•´ì ¸ ìˆì§€ ì•Šê³ , ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì˜ë¯¸ ìˆëŠ” ì •ë³´ë§Œ ë½‘ì•„ì¤˜.
"key: value" í˜•ì‹ìœ¼ë¡œë§Œ ì •ë¦¬í•´ì¤˜.
í‘œ êµ¬ì¡°ë¥¼ ì´í•´í•´ì„œ ê° í•­ëª©ì˜ ë³€ê²½ ì „/í›„ ë‚´ìš©ë„ í•¨ê»˜ ê³ ë ¤í•˜ê³ ,í•µì‹¬ì ì¸ ê°’ë“¤ë§Œ ëª…í™•í•œ key-value í˜•íƒœë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”.
í‘œì—ì„œ ë³€ê²½ì‚¬í•­ì´ ìˆëŠ” ê²½ìš°ëŠ” í™”ì‚´í‘œë¥¼ ì´ìš©í•´ì„œ ê°€ì‹œì„± ì¢‹ê²Œ ë§Œë“¤ì–´ì¤˜
í‘œë‚˜ ë‚´ìš©ì´ ê³µë€ì¸ ê²½ìš°ëŠ” ê³µë€ìœ¼ë¡œ í‘œì‹œí•´ì£¼ë©´ ë¼

ğŸ“Œ í‘œ (HTML):
{html_text}

ğŸ“Œ OCR í…ìŠ¤íŠ¸ ë°•ìŠ¤:
{ocr_text_block}
"""

response = openai_client.chat.completions.create(
    model="gpt-4.1",  # ë˜ëŠ” "gpt-4.1"
    messages=[{"role": "user", "content": prompt}],
    temperature=0.3
)

gpt_output = response.choices[0].message.content
# ** ê°•ì¡° ê¸°í˜¸ ì œê±°
gpt_output = re.sub(r"\*\*", "", gpt_output)

print("ğŸ§¾ GPT ì‘ë‹µ:")
print(gpt_output)

# === key-value ì¶”ì¶œ í•¨ìˆ˜ ===
def extract_key_value_pairs(text):
    lines = text.strip().split('\n')
    pairs = []
    for line in lines:
        if ":" in line:
            key, value = line.split(":", 1)
            key = key.strip()

            value = value.strip()
            if key and value:
                pairs.append((key, value))
    return pairs

key_value_list = extract_key_value_pairs(gpt_output)
df = pd.DataFrame(key_value_list, columns=["Key", "Value"])

# === ì—‘ì…€ ì €ì¥ ===
base_name = os.path.splitext(os.path.basename(input_image_path))[0]
excel_save_path = os.path.join(save_folder, f"{base_name}_key_value.xlsx")
df.to_excel(excel_save_path, index=False)
print(f"âœ… ì—‘ì…€ ì €ì¥ ì™„ë£Œ: {excel_save_path}")
