import tritonclient.http as httpclient
import numpy as np
import os
import base64
from PIL import Image
from openai import OpenAI
from dotenv import load_dotenv
import pandas as pd
import re

# === ì„¤ì • ===
TRITON_URL = "localhost:8000"
MODEL_NAME = "ensemble"
input_image_path = "/workspace/sample_image2/insure/images/insure_00004.jpeg"
#input_image_path = "/workspace/sample_images/IMG_OCR_6_F_0015610.png"
save_folder = "/workspace/result"
os.makedirs(save_folder, exist_ok=True)

# Triton í´ë¼ì´ì–¸íŠ¸
client = httpclient.InferenceServerClient(url=TRITON_URL)

def encode_image_base64(image_path):
    with open(image_path, "rb") as f:
        encoded = base64.b64encode(f.read()).decode("utf-8")
        if len(encoded) % 4:
            encoded += "=" * (4 - len(encoded) % 4)
        return encoded

def decode_float(x):
    try:
        return float(x.decode("utf-8")) if isinstance(x, bytes) else float(x)
    except:
        return 0.0

# Triton ì¶”ë¡ 
encoded_image = encode_image_base64(input_image_path)
inputs = httpclient.InferInput("input_image", [1], "BYTES")
inputs.set_data_from_numpy(np.array([encoded_image], dtype=object))
outputs = [
    httpclient.InferRequestedOutput("final_texts_with_line"),
    httpclient.InferRequestedOutput("html_tags")
]
response = client.infer(model_name=MODEL_NAME, inputs=[inputs], outputs=outputs)
texts = response.as_numpy("final_texts_with_line")
html = response.as_numpy("html_tags")

# OCR í…ìŠ¤íŠ¸ ì •ë¦¬
ocr_lines = []
for t in texts:
    x1, y1, x2, y2, text, line_num = t
    x1, y1, x2, y2 = map(decode_float, [x1, y1, x2, y2])
    text = text.decode("utf-8") if isinstance(text, bytes) else str(text)
    ocr_lines.append(f"[{x1:.1f}, {y1:.1f}, {x2:.1f}, {y2:.1f}] \"{text}\"")
ocr_text_block = "\n".join(ocr_lines)
html_text = "\n".join(line.decode("utf-8") for line in html)

# OpenAI
load_dotenv()
openai_client = OpenAI()

# GPT Vision í”„ë¡¬í”„íŠ¸
prompt = f"""
ë‹¤ìŒì€ ì‹ ì²­ì„œ ë¬¸ì„œì—ì„œ ì¶”ì¶œëœ í‘œì™€ OCR í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤.

ì´ ë¬¸ì„œì—ì„œ ì˜ë¯¸ ìˆëŠ” key-value ì •ë³´ë¥¼ ì •ë¦¬í•´ì£¼ì„¸ìš”.
ê° í•­ëª©ì€ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”:

key: value

í‘œì˜ êµ¬ì¡°ì™€ OCR í…ìŠ¤íŠ¸, ì´ë¯¸ì§€ ìì²´ë¥¼ ëª¨ë‘ ê³ ë ¤í•´ì„œ ì¤‘ìš”í•œ ì •ë³´ë§Œ ë½‘ì•„ì£¼ì„¸ìš”.
ë³€ê²½ ì „/í›„ ì •ë³´ê°€ ìˆë‹¤ë©´ 'â†’' ê¸°í˜¸ë¡œ êµ¬ë¶„í•´ì£¼ì„¸ìš”.
ì´ë¯¸ì§€ì˜ ì†ê¸€ì”¨ ì •ë³´ë¥¼ ê³ ë ¤í•´ì„œ valueê°’ì„ ì •í•´ì£¼ì„¸ìš” 
ê´„í˜¸ ê³µë€, ì„œëª…ë€ ë“±ì´ ìˆëŠ” ê²½ìš°ëŠ”, ê·¸ ë¶€ë¶„ë“¤ë§Œ ëº€ ë‹¨ì–´ë“¤ë¡œ ê³ ë ¤í•´ì£¼ì„¸ìš”
"""

# GPT-4 Vision ìš”ì²­
with open(input_image_path, "rb") as image_file:
    response = openai_client.chat.completions.create(
        model="gpt-4.1",
        messages=[
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": prompt},
                    {"type": "image_url", "image_url": {
                        "url": f"data:image/png;base64,{encoded_image}"
                    }},
                    {"type": "text", "text": "\nğŸ“Œ HTML í…Œì´ë¸”:\n" + html_text},
                    {"type": "text", "text": "\nğŸ“Œ OCR í…ìŠ¤íŠ¸:\n" + ocr_text_block},
                ]
            }
        ],
        temperature=0.3,
    )

# ì‘ë‹µ íŒŒì‹±
gpt_output = response.choices[0].message.content
gpt_output = re.sub(r"\*\*", "", gpt_output)

print("ğŸ§¾ GPT ì‘ë‹µ:")
print(gpt_output)

# key-value ì¶”ì¶œ
def extract_key_value_pairs(text):
    lines = text.strip().split('\n')
    pairs = []
    for line in lines:
        if ":" in line:
            key, value = line.split(":", 1)
            key = key.strip()
            value = value.strip()
            if key and value:
                pairs.append((key, value))
    return pairs

key_value_list = extract_key_value_pairs(gpt_output)
df = pd.DataFrame(key_value_list, columns=["Key", "Value"])

# ì—‘ì…€ ì €ì¥
base_name = os.path.splitext(os.path.basename(input_image_path))[0]
excel_save_path = os.path.join(save_folder, f"{base_name}_key_value.xlsx")
df.to_excel(excel_save_path, index=False)
print(f"âœ… ì—‘ì…€ ì €ì¥ ì™„ë£Œ: {excel_save_path}")
